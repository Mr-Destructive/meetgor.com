{
  "title": "Techstructive Weekly #38",
  "post_dir": "newsletter",
  "type": "newsletter",
  "status": "published",
  "slug": "techstructive-weekly-38",
  "date": "2025-04-19"
}

## Week #38

A week after ages, I finally had some energy, some spark back. I am actually overwhelmed with this AI revolution, too many ideas, too little mental space, its just moving too fast. Like I feel, I was just in this newsletter yesterday, and here I am a week gone with so many things launched, dropped, and bomb shelled.

### Speed in the AI Revolution

Yes, AI launches feel like a bomb shell being dropped, in a good way, but can someone tell these AI companies to take a breath? At this pace, we all will be juggling models all the time. New models every week, deprecation of previous models, new ways to interact with tools, it’s just creating an endless cycle of progress. Yes, we might be progressing in better human intelligence,e but at the cost of letting too many possibilities just dying out due to speed.

Thanks for reading Techstructive Weekly! Subscribe for free to receive new posts and support my work.

Speed is good, but over-speeding or over-of-anything is bad. And the AI revolution is just too speedy, its not letting people take time and create, it’s bombarding with its possibilities. I might be wrong here, you might see very tiny progress being made, yes that is also true, but the amount of content being generated around this and filtering the noise is too hard right now.

Anyways, enough of that big ramble, I might continue this in a separate post altogether where I will distill this thought.

### Personal Update

On the personal update side, I was full of energy this week. Open-AI just partnered with WindSurf to provide the 4.1 models access for free for a week. And I was not going to miss out on that, so I gave it a shot at vibe coding. it was fun to say the least, it did everything (rightly, maybe not completely though). This vibe coding is not bad if you know what you are doing, and at times, at AI fails, you can always steer it.

I also edited and recorded a mammoth 52-minute video, a one-shot voice-over. That brings me joy, from 6 6-minute video taking 3 hours to 1 1-hour video taking just over an hour to add voice over, that is a win for me.

### Vibe Coding Experience

I did some digging in the codex cli, I gave windsurf a shot to create this cli in Python, and it did, but it was not what I expected; it just wrapped the API call in a cli. That also might have taken me a few hours, but AI did that in a few minutes, impressive stuff. It’s not like waiting and staring at it code, but rather, nudge, debug, and steer the conversation to progress. It’s more of you driving it to do what you want to do and let it handle the decisions to reach there, no, how granular you want the autonomy that you can always do, so:

- Lesser autonomy> Use inline completion that Tab tab thing, it’s kind of good at doing that
- More Autonomy> Use the chat interface to talk through the solution to it and let it create. Here, it’s not that great. The more specific you get, the less it grasps. The coherence is a little dicey here
    - Maybe this is good for an initial prototype. It removes that staring at a blank page, you will have something to fix, something to fight the way for

This needs more time and experience to comment on, but I have been doing a similar thing at work, and most of the time it’s me who has given the direction for the LLM to take, and it's helping me find out things and narrow down my search space and effort.

Overall, the week was pretty refreshing and exciting. Learnt a lot of things, which you can read more about.

### Quote of the week

> **"If everything is important, then nothing is."**
> 
> — **Patrick Lencioni**

In this AI hype, everything seems to draw attention, and after attention is all you need.

But is it? Every model seems to be different in some or the other way, however good or bad it may be. In the last month itself, there are so many things launched, revealed, and invented that there is more than a year's worth of discovery to be done, let alone in the past year, we have decades worth of rundown content with us. But it’s just too unstable and fast.

If every AI company thinks their model is important and ground-breaking or SOTA (state of the art) for each of their new models, then I think none of them really are.

Isn’t that part clear to them, or are they just too busy to keep up with the AI Race? The latter seems likely, but who am I to comment on that? I can take my time to absorb the things happening and filter out the things myself from the noise and potential unimportant buzz and hype (maybe important later) out there.

---

## Created

- [Appwrite Cloud Function Setup, creation, and deployment in Golang](https://www.youtube.com/watch?v=dq9czq0JgZA)
  I recorded a tutorial on setting up a cloud function, creating the function in golang, and deploying it to Appwrite Cloud via
    - CLI
    - Git
    - Manual (gzip file)Double click to interact with video

## Read

- [How to build an Agent](https://ampcode.com/how-to-build-an-agent)
  This was a simple article, but it’s so important to read. The things that we see around sometimes feel magical, people might just stare at them and remain in awe for eternity. However, people like engineers and Thorsten Ball (he is an author and engineer), who are fueled not just by skill but also by curiosity and the will to go a step ahead, maybe even two steps. Those types of people understand the craft, play with it (maybe fail several times), gain clarity, gain wisdom, and not just for selfish greed or promotion, they are wise and generous, they share it with others. Those are not humans, those are angels’ inspiration for all of us (at least for me)
  Agents are not magic; they are LLMs with tools, and they can predict which word(token) to insert next in the sentence. Apply this to hand them the problem and some tools (functions, api calls, documentation, database), and they can understand what to look up for, what to do with it next, get the result, and decide again, in a loop. Just like that. Agents are just loops over an LLM call (at least in the broader way).
- [Our best customers are now Robots](https://fly.io/blog/fuckin-robots/)
  > UX > DX > RX
  A nice way to put that
    - We first cared about **User experience** with HTML and JavaScript.
    - Then **Developer experience** with React and god knows what frameworks
    - Now, **Robots’ experience** with LLMs in the picture, we have MCP, A2A, and again,n god knows what will drop tomorrow

  Tech is changing fast as hell.
- [The Best Programmer I know](https://endler.dev/2025/best-programmers/)

It is a manual on how to become a better programmer, I can really stick the headings of the post a goals to improve as a developer.

- [Believe it’s going to work even though it probably won’t](https://world.hey.com/dhh/believe-it-s-going-to-work-even-though-it-probably-won-t-58fd9dc5)
  Startup founders are strong-headed, which is what I also agree upon. And like everything in life, it is a double-edged sword. Everything has pros and cons, but to not be cut by the other edge, you have to sharpen the other; you can’t dismiss the other edge but keep on sharpening the other one.
  Sorry, that was a weird tangent. Startup founders need to be strong-headed because it’s their dream to build and grow; if they stop defending themselves, then the team just disintegrates.

## Watched

- Raising an Agent [Episodes 1](https://www.youtube.com/watch?v=Cor-t9xC1ck) and [2](https://www.youtube.com/watch?v=5-LPfATZjyM)
  Sourcegraph is such an open company, open as in the openness about their thinking and approaches. They are heading in a good direction, I think and they might be a fundamental step in something that no one else is noticing yet. They are taking the time and not rushing through the model race. I am betting on them now, a comeback is right around the corner.Double click to interact with video

- [Firebase makes an AI IDE? Firebase Studio](https://youtu.be/ljW1smUpa2U?si=liUDLrETTSw3y02c)
  App builders are good, but not there yet.
  > Authentication, coming soon!
  That was a hilarious dev-like builder. So relatable.
  It now makes sense, as it is trained on real developers’ data, so if you know, you know.
  And that brings a relief as well, as to the question of it replacing developers. That’s just not happening, as developers have not yet figured out their lives yet.Double click to interact with video

## Learnt

- Please use f-strings instead of old format strings in Python: Save yourself from questioning your existence
  Man, this bug is lethal. If you have lets say a logger function that takes in a log string and arguments. You might be questioning your existence in debugging those errors. Since this is pretty nasty to use in logs, as we tend to looks for logs to find the errors. But if the logs itself are causing the issue, then good luck in finding where on earth the code is failing. Please use f-strings and save yourself from this madness `TypeError: not enough arguments for format string`
- Vibe coding with Windsurf
    - Windsurf let’s you prompt and generate, edit and delete files, execute commands and even debugging the errors.
    - Its a good place to start, and then take control yourself. I just found it best for getting the boilerplate (too basic CRUD) steps and the UI parts. For other complex operations, it can’t vibe yet.
- Intuition Vibe debugging with Chat GPT
    - I had this above bug and there was try catch expression. The error must be in the middle code of it, but the logs that should have been there weren’t there. That made me wonder what the heck is happening.
    - I tried to replicate the issue locally and found that we weren’t using the standard logger; it was a custom implementation of a logger. And then I suspected that is where it might be breaking.
    - Then I went a bit deeper in the implementation of the code and asked gpt to verify that the place I suspect the issue is. And I was correct.
    - This is am terming as Intuition vibe debugging, as a developer, you know what to expect and what not to, you can save time (and right now tokens) for LLM to do that for you, you can narrow down the search space and let it verify your intuitions. A lethal combination so far.
    - I have not only used it once or twice, but several times, while checking the accuracy of ground truth and predictions, analyzing logs, filtering issues among the noise, and so many another trivial but too manual kind of operations that consume time and energy.

## Tech News

AI is truly breaking the world, every day there is a new model, new shiny tools, new way to approach things. So much so that it has become really overwhelming to take a deep breathe and listen, observe, learn in this cluttered and shiny, noisy world.

- Open AI releases 4.1 4.1-mini, 4.1-nano breaking the accuracy of 4.5
- Open AI also releases o3 and o4-mini
- Google releases 2.5 Flash

Next week, Antrophic will release something, that is my forecast, because we haven’t witnessed a week where there has been without a model release. I am excited but overwhelmed and scared as shit at the same time.

Let us breathe!

For more interesting articles, check out the [hackernewsletter](https://buttondown.com/hacker-newsletter/archive/hacker-newsletter-742) for the week edition [#742](https://buttondown.com/hacker-newsletter/archive/hacker-newsletter-742), for even more software development/coding articles, join [daily.dev](http://daily.dev/).

[Leave a comment](%%half_magic_comments_url%%)

Thanks for reading Techstructive Weekly! This post is public so feel free to share it.

[Share](%%share_url%%)

---

That’s it from this edition of the newsletter, see you next week (hopefully no AI releases this week, like just stop for a week)

Happy Coding :) (maybe happy vibing now)

Thanks for reading Techstructive Weekly! Subscribe for free to receive new posts and support my work.
